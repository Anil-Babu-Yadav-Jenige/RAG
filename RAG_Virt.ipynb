{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOELis5XgxThWAVDMMxetXY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anil-Babu-Yadav-Jenige/RAG/blob/main/RAG_Virt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eaqP5nTl69Y7"
      },
      "outputs": [],
      "source": [
        "# perform google colab installs\n",
        "import os\n",
        "\n",
        "if \"COLAB_GPU\" in os.environ:\n",
        "    print(\"[INFO] Running in Google Colab, installing requirements.\")\n",
        "    #!pip install -U torch  #requires torch 2.1.1+ (for efficient sdpa implementation)\n",
        "    !pip install PyMuPDF # for reading pdfs with python\n",
        "    !pip install tqdm #for progress bars\n",
        "    !pip install accelerate # for quatization model loading\n",
        "    !pip install bitsandbytes # for quantising models (less storage space)\n",
        "    !pip install flash-attn --no-build-isolation # for faster attention mechanisms = faster LLM inference\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -y torch torchvision torchaudio transformers sentence-transformers\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "!pip install -U transformers sentence-transformers"
      ],
      "metadata": {
        "id": "CUobB7_-4nyg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.Document/Text Procesing and Embedding Creation"
      ],
      "metadata": {
        "id": "nc0jETL6Penu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Download pdf file\n",
        "import os\n",
        "import requests\n",
        "\n",
        "# get pdf document\n",
        "pdf_path = \"human-nutrition-text.pdf\"\n",
        "\n",
        "#download pdf if it does not already exists\n",
        "if not os.path.exists(pdf_path):\n",
        "  print(\"file doesn't exist, downloading...\")\n",
        "\n",
        "  #the URL of the pdf\n",
        "  #url = \"https://pressbooks.oer.hawaii.edu/humannutristion2/open/download?type=pdf\"\n",
        "  #url = \"https://pressbooks.oer.hawaii.edu/humannutrition2/\"\n",
        "  url = \"https://pressbooks.oer.hawaii.edu/humannutrition2/open/download?type=pdf\"\n",
        "\n",
        "\n",
        "  #the local file name to save the downloaded file\n",
        "  filename = pdf_path\n",
        "\n",
        "  #send a GET request to the url\n",
        "  response = requests.get(url)\n",
        "\n",
        "  #check if the request was successful\n",
        "  if response.status_code == 200:\n",
        "      #open a file in binary write mode and save the content to it\n",
        "      with open(filename, \"wb\") as file:\n",
        "          file.write(response.content)\n",
        "      print(f\"file downloaded successfully and saved as {filename}\")\n",
        "  else:\n",
        "      print(\"failed to download the file. Status code: {response.status_code}\")\n",
        "else:\n",
        "  print(f\"file {pdf_path} already exists.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "YKVlSGx556FA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read the pdf"
      ],
      "metadata": {
        "id": "QANumQ0-WICX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Requires !pip install PyMupdf, see pymupdf git hub\n",
        "!pip install pymupdf\n",
        "import fitz # (pymupdf, found this is better that pypdf for our use case) loads pymu package\n",
        "from tqdm.auto import tqdm # for progress bars, requires !pip install tqdm\n",
        "\n",
        "def text_formatter(text: str) -> str: #tf make sure no empty spaces\n",
        "  \"\"\"Perform minor formatting on text,\"\"\"\n",
        "  cleaned_text = text.replace(\"\\n\", \" \").strip() # note: this might be different\n",
        "\n",
        "  #other potential text formatting functions can go here\n",
        "  return cleaned_text\n",
        "\n",
        "#open pdf and get lines/pages\n",
        "#note this only focuses on text, rather than images/figures etc\n",
        "def open_and_read_pdf(pdf_path: str) -> list[dict]:\n",
        "  \"\"\"\n",
        "  Opens a pdf file, reads its text context page by page, and collects statistics.\n",
        "\n",
        "  Parameters:\n",
        "      pdf_path (str): The path to the PDF file to be opend and read.\n",
        "\n",
        "  Returns:\n",
        "      list[dict]: A list of dictionaries, each containing the page number(adjusted), character count, word count, sentence count, token count, and the extracted text for each page.\n",
        "  \"\"\"\n",
        "  doc = fitz.open(pdf_path) #open pdf documet\n",
        "  pages_and_texts = [] # maintaing a list with this name, & each elemnt of this list is a dictionary\n",
        "  #1st element of this list is page 1 wjhich contains all specified\n",
        "  for page_number, page in tqdm(enumerate(doc)): #iterate the document pages, go through every page\n",
        "      text = page.get_text()  #get plain text encoded as utf-8\n",
        "      text = text_formatter(text) # remove empty spaces SEE NOTES\n",
        "      pages_and_texts.append({\"page_number\": page_number -41, #adjust page numbers since our pdf, the page where book actually starts }\n",
        "                              \"page_char_count\": len(text),\n",
        "                              \"page_word_count\": len(text.split()),\n",
        "                              \"page_sentence_count_raw\": len(text.split(\".\")),\n",
        "                              \"page_token_count\": len(text) / 4, # 1 token =~ 4 char\n",
        "                              \"text\": text})\n",
        "  return pages_and_texts\n",
        "\n",
        "pages_and_texts = open_and_read_pdf(pdf_path=pdf_path)\n",
        "pages_and_texts[:2] #randomly print out 2 dictionaries"
      ],
      "metadata": {
        "id": "ls9FL9gG56Dx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now lets get a random sample of the pages"
      ],
      "metadata": {
        "id": "x5AoUlG0e9sn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "random.sample(pages_and_texts, k=3)"
      ],
      "metadata": {
        "id": "3d0ohVWUfDIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get some statistics"
      ],
      "metadata": {
        "id": "CU3J0ST_gOzH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame(pages_and_texts)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "G3sKtMg2gRnn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Over all Statistics"
      ],
      "metadata": {
        "id": "b-hB4ZrNhkRX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Get status\n",
        "df.describe().round(2)\n",
        "\n",
        "# why are we looking at number tokens per page- to check for the context window size\n",
        "#"
      ],
      "metadata": {
        "id": "9tk8U08zhnoo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RAG Chunking Strategies.**"
      ],
      "metadata": {
        "id": "ZhGTUb60rYou"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 0: Load packages\n",
        "\n",
        "\n",
        "Step 1:Document processing\n",
        "\n",
        "\n",
        "Step 2:Reading the document."
      ],
      "metadata": {
        "id": "39yr7Qytr3-c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Testing 5 chunking startegies:fixed, recursive, semantic, structural and LLM vased.\n",
        "\n",
        "**Chunking startegy 1: Fixed size chunking.**"
      ],
      "metadata": {
        "id": "b6nezjGXs6Lk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chunk_text(text: str, chunk_size: int = 500) -> list:\n",
        "    \"\"\"\n",
        "    Splits a given text into chunks of a specified size.\n",
        "    \"\"\"\n",
        "    chunks = []\n",
        "    words = text.split()\n",
        "    current_chunk = \"\"\n",
        "\n",
        "    for word in words:\n",
        "        #check if adding the word exceeds chunk size\n",
        "        if len(current_chunk) + len(word) + 1 <= chunk_size:\n",
        "            current_chunk += (word + ' ')\n",
        "        else:\n",
        "            #store current chunk and start new one\n",
        "            chunks.append(current_chunk.strip())\n",
        "            current_chunk = word + ' '\n",
        "\n",
        "    #add the last chunk\n",
        "    if current_chunk:\n",
        "        chunks.append(current_chunk.strip())\n",
        "    return chunks\n",
        "\n",
        "\n",
        "def chunk_pdf_pages(pages_and_texts: list, chunk_size: int = 500) -> list[dict]:\n",
        "    \"\"\"\n",
        "    Takes pdfpages with text and splits them into chunks\n",
        "\n",
        "    Returns a list of dicts with page_number, chunk_index, and chunk_text:\n",
        "    \"\"\"\n",
        "    all_chunks = []\n",
        "    for page in pages_and_texts:\n",
        "        page_text = page[\"text\"]\n",
        "        page_number = page[\"page_number\"]\n",
        "\n",
        "        chunks = chunk_text(page_text, chunk_size=chunk_size) # pass text of a page to chunk_text function\n",
        "        for i, chunk in enumerate(chunks):\n",
        "            all_chunks.append({\"page_number\": page_number,\n",
        "                           \"chunk_index\": i,\n",
        "                           \"chunk_char_count\": len(chunk),\n",
        "                           \"chunk_word_count\": len(chunk.split()),\n",
        "                           \"chunk_token_count\": len(chunk)/4, # rough token estimate\n",
        "                           \"chunk_text\": chunk\n",
        "            })\n",
        "    return all_chunks\n",
        "\n",
        "#example usage\n",
        "chunked_pages = chunk_pdf_pages(pages_and_texts, chunk_size=500)\n",
        "print(f\"Total chunks: {len(chunked_pages)}\")\n",
        "print(f\"First chunk (page {chunked_pages[0]['page_number']}): {chunked_pages[0]['chunk_text'][:200]}...\")"
      ],
      "metadata": {
        "id": "3WEkePotrdSk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After this , want to visualize how every chunk actually look like."
      ],
      "metadata": {
        "id": "l41aTNs54KKE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random, textwrap # Import necessary modules: 'random' for sampling, 'textwrap' for text formatting\n",
        "\n",
        "#---Sampling & Pretty Prinnting---\n",
        "def _scattered_indices(n: int, k: int, jitter_frac: float = 0.08) -> list[int]:\n",
        "    \"\"\"Evenly spaced anchors + random jitter -indices scattered across [0, n-1], \"\"\"\n",
        "    # Check if we need to select any indices. If k (number of indices) is zero or less, return an empty list.\n",
        "    if k <= 0:\n",
        "      return []\n",
        "    # If we only need one index, pick a single random index from 0 to n-1.\n",
        "    if k == 1:\n",
        "      return [random.randrange(n)]\n",
        "    # Calculate k evenly spaced anchor points across the range [0, n-1].\n",
        "    anchors = [int(round(i * (n-1) / (k-1))) for i in range(k)]\n",
        "    out, seen = [], set()\n",
        "    radius = max(1, int(n * jitter_frac))\n",
        "\n",
        "    # Iterate through each anchor point to add random jitter.\n",
        "    for a in anchors:\n",
        "        # Define the lower (lo) and upper (hi) bounds for the random jitter, constrained by [0, n-1].\n",
        "        lo, hi = max(0, a- radius), min(n-1, a+ radius)\n",
        "        j = random.randint(lo, hi)\n",
        "\n",
        "        # If the jittered index 'j' hasn't been used yet, add it to the output and the 'seen' set.\n",
        "        if j not in seen:\n",
        "            out.append(j); seen.add(j)\n",
        "    # If the process above resulted in fewer than k unique indices (due to jitter overlap), fill the rest randomly.\n",
        "    while len(out) < k:\n",
        "        r = random.randrange(n) # Pick a random index from the whole range [0, n-1].\n",
        "        # If the index hasn't been used, add it to the output.\n",
        "        if r not in seen:\n",
        "            out.append(r); seen.add(r)\n",
        "    return out\n",
        "\n",
        "def _draw_boxed_chunk(c: dict, wrap_at: int = 96) -> str:\n",
        "    header = (\n",
        "        f\"Page p{c['page_number']} . idx {c['chunk_index']} | \"\n",
        "        f\"chunk {c['chunk_char_count']} . words {c['chunk_word_count']} . ~tokens {c['chunk_token_count']} \"\n",
        "    )\n",
        "    #wrap body text, avoid breaking long words awkwardly\n",
        "    wrapped_lines = textwrap.wrap(c[\"chunk_text\"], width=wrap_at, break_long_words=False, replace_whitespace=False\n",
        "                                  )\n",
        "    context_width = max( [0, *map(len, wrapped_lines)])\n",
        "    box_width = max(len(header), context_width + 2) # +2 for side padding\n",
        "\n",
        "    # Transcribed code starts here:\n",
        "    top = \"╔\" + \"=\" * box_width + \"╗\"\n",
        "    hline = \"║ \" + header.ljust(box_width) + \" ║\"\n",
        "    sep = \"╠\" + \"-\" * box_width + \"╣\"\n",
        "    body = \"\\n\".join(\n",
        "        \"║ \" + line.ljust(box_width - 2) + \" ║\" for line in wrapped_lines) or (\"║ \" + \"\".ljust(box_width- 2) + \" ║\")\n",
        "    bottom = \"╚\" + \"=\" * box_width + \"╝\"\n",
        "    return \"\\n\".join([top, hline, sep, body, bottom])\n",
        "def show_random_chunks(pages_and_texts: list, chunk_size: int = 500, k: int = 5, seed: int | None = 42):\n",
        "    if seed is not None:\n",
        "       random.seed(seed)\n",
        "    all_chunks = chunk_pdf_pages(pages_and_texts, chunk_size=chunk_size)\n",
        "    if not all_chunks:\n",
        "        print(\"No chunks to display.\")\n",
        "        return\n",
        "    idxs = _scattered_indices(len(all_chunks), k)\n",
        "    print(f\"Showing {len(idxs)} random chunks out of {len(all_chunks)} total:\\n\")\n",
        "    for i, idx in enumerate(idxs, 1):\n",
        "        print(f\"#{i}\")\n",
        "        print(_draw_boxed_chunk(all_chunks[idx]))\n",
        "        print()\n",
        "\n",
        "# run\n",
        "assert 'pages_and_texts' in globals(), \"Run: pages_and_texts = open_and_read_pdf(pdf_path) first.\"\n",
        "show_random_chunks(pages_and_texts, chunk_size=500, k=5, seed=42)"
      ],
      "metadata": {
        "id": "xPL4S4uW4Rip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Chunking Stratergy 2: Semantic chunking**"
      ],
      "metadata": {
        "id": "7Wh0G_jkgc-X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install --upgrade \"sentence-transformers==3.0.1\" \"transformers<5,>4.41\" scikit-learn nltk"
      ],
      "metadata": {
        "id": "cBr1GJACiQVH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer # Correct import for SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('punkt', quiet=True)\n",
        "\n",
        "#load once locally\n",
        "semantic_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "def semantic_chunk_text(text: str, similarity_threshold: float = 0.8, max_tokens: int = 500) -> list:\n",
        "    \"\"\"\n",
        "    splits text into semantic chunks based on sentence similarity and max token lenght.\n",
        "    \"\"\"\n",
        "\n",
        "    sentences = nltk.sent_tokenize(text) #break the page into sentences\n",
        "    if not sentences:\n",
        "        return []\n",
        "\n",
        "    embeddings = semantic_model.encode(sentences)\n",
        "\n",
        "    chunks = []\n",
        "    current_chunk = [sentences[0]] #keep on appending to the current chunk until the similarity score is > than the threshold\n",
        "    current_embedding = embeddings[0]\n",
        "\n",
        "    for i in range(1, len(sentences)):\n",
        "        sim = cosine_similarity([current_embedding], [embeddings[i]])[0][0]\n",
        "        chunk_token_count = len(\" \".join(current_chunk)) // 4\n",
        "\n",
        "        if sim >= similarity_threshold and chunk_token_count <= max_tokens:\n",
        "            current_chunk.append(sentences[i])\n",
        "            current_embedding = np.mean([current_embedding, embeddings[i]], axis=0)\n",
        "        else: #if not > breakout\n",
        "            chunks.append(\" \".join(current_chunk))\n",
        "            current_chunk = [sentences[i]] # Corrected variable name from sentence to sentences\n",
        "            current_embedding = embeddings[i]\n",
        "\n",
        "    if current_chunk:\n",
        "        chunks.append(\" \".join(current_chunk))\n",
        "\n",
        "    return chunks\n",
        "\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "def semantic_chunk_pdf_pages(pages_and_texts: list, similarity_threshold: float = 0.8, max_tokens: int = 500) -> list[dict]:\n",
        "\n",
        "  \"\"\"takes pdf pages with text and splits them into semantic chunks.\n",
        "\n",
        "  ReturNS A LIST OF DICTS WITH PAGE_NUMBER, CHUNK_INDEX, AND CHUNK_TEXT:\n",
        "  \"\"\"\n",
        "  all_chunks = []\n",
        "\n",
        "  for page in tqdm(pages_and_texts, desc=\"Semantic chunking pages\"): # Corrected capitalization of Tqdm\n",
        "      page_text = page[\"text\"]\n",
        "      page_number = page[\"page_number\"]\n",
        "\n",
        "      chunks = semantic_chunk_text(page_text, similarity_threshold=similarity_threshold, max_tokens=max_tokens)\n",
        "\n",
        "      for i, chunk in enumerate(chunks):\n",
        "          all_chunks.append({\n",
        "              \"page_number\": page_number, \"chunk_index\": i,\n",
        "              \"chunk_char_count\": len(chunk),\n",
        "              \"chunk_word_count\": len(chunk.split()),\n",
        "              \"chunk_token_count\": len(chunk) // 4,\n",
        "              \"chunk_text\": chunk\n",
        "          })\n",
        "  return all_chunks"
      ],
      "metadata": {
        "id": "b7rBWe7JjjiZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "semantic_chunked_pages = semantic_chunk_pdf_pages(pages_and_texts, similarity_threshold=0.75, max_tokens= 500)\n",
        "print(f\"Total semantic chunks: {len(semantic_chunked_pages)}\")\n",
        "print(f\"First semantic chunk (page{semantic_chunked_pages[0]['page_number']}):\")\n",
        "print(semantic_chunked_pages[0]['chunk_text'][:200] + \"...\")"
      ],
      "metadata": {
        "id": "fGCLGgyCrEZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import textwrap # Import necessary modules: 'random' for sampling, 'textwrap' for text formatting\n",
        "\n",
        "#---Sampling & Pretty Prinnting---\n",
        "def _scattered_indices(n: int, k: int, jitter_frac: float = 0.08) -> list[int]:\n",
        "    \"\"\"Evenly spaced anchors + random jitter -indices scattered across [0, n-1], \"\"\"\n",
        "    # Check if we need to select any indices. If k (number of indices) is zero or less, return an empty list.\n",
        "    if k <= 0:\n",
        "      return []\n",
        "    # If we only need one index, pick a single random index from 0 to n-1.\n",
        "    if k == 1:\n",
        "      return [random.randrange(n)]\n",
        "    # Calculate k evenly spaced anchor points across the range [0, n-1].\n",
        "    anchors = [int(round(i * (n-1) / (k-1))) for i in range(k)]\n",
        "    out, seen = [], set()\n",
        "    radius = max(1, int(n * jitter_frac))\n",
        "\n",
        "    # Iterate through each anchor point to add random jitter.\n",
        "    for a in anchors:\n",
        "        # Define the lower (lo) and upper (hi) bounds for the random jitter, constrained by [0, n-1].\n",
        "        lo, hi = max(0, a- radius), min(n-1, a+ radius)\n",
        "        j = random.randint(lo, hi)\n",
        "\n",
        "        # If the jittered index 'j' hasn't been used yet, add it to the output and the 'seen' set.\n",
        "        if j not in seen:\n",
        "            out.append(j); seen.add(j)\n",
        "    # If the process above resulted in fewer than k unique indices (due to jitter overlap), fill the rest randomly.\n",
        "    while len(out) < k:\n",
        "        r = random.randrange(n) # Pick a random index from the whole range [0, n-1].\n",
        "        # If the index hasn't been used, add it to the output.\n",
        "        if r not in seen:\n",
        "            out.append(r); seen.add(r)\n",
        "    return out\n",
        "\n",
        "def _draw_boxed_chunk(c: dict, wrap_at: int = 96) -> str:\n",
        "    header = (\n",
        "        f\"Page p{c['page_number']} . idx {c['chunk_index']} | \"\n",
        "        f\"chunk {c['chunk_char_count']} . words {c['chunk_word_count']} . ~tokens {c['chunk_token_count']} \"\n",
        "    )\n",
        "    #wrap body text, avoid breaking long words awkwardly\n",
        "    wrapped_lines = textwrap.wrap(c[\"chunk_text\"], width=wrap_at, break_long_words=False, replace_whitespace=False\n",
        "                                  )\n",
        "    context_width = max( [0, *map(len, wrapped_lines)])\n",
        "    box_width = max(len(header), context_width + 2) # +2 for side padding\n",
        "\n",
        "    # Transcribed code starts here:\n",
        "    top = \"╔\" + \"=\" * box_width + \"╗\"\n",
        "    hline = \"║ \" + header.ljust(box_width) + \" ║\"\n",
        "    sep = \"╠\" + \"-\" * box_width + \"╣\"\n",
        "    body = \"\\n\".join(\n",
        "        \"║ \" + line.ljust(box_width - 2) + \" ║\" for line in wrapped_lines) or (\"║ \" + \"\".ljust(box_width- 2) + \" ║\")\n",
        "    bottom = \"╚\" + \"=\" * box_width + \"╝\"\n",
        "    return \"\\n\".join([top, hline, sep, body, bottom])\n",
        "def show_random_semantic_chunks(semantic_chunked_pages: list[dict], k: int = 5, seed: int | None = 42):\n",
        "    if seed is not None:\n",
        "       random.seed(seed)\n",
        "    n = len(semantic_chunked_pages)\n",
        "    if n == 0:\n",
        "        print(\"No semantic chunks to display.\");\n",
        "        return\n",
        "    idxs = _scattered_indices(n, k)\n",
        "    print(f\"Showing {len(idxs)} scattered random SEMANTIC chunks out of {n} total:\\n\")\n",
        "    for i, idx in enumerate(idxs, 1):\n",
        "        print(f\"#{i}\")\n",
        "        print(_draw_boxed_chunk(semantic_chunked_pages[idx]))\n",
        "        print()\n",
        "\n",
        "# run (expects youo have already created semantic chunked pages)\n",
        "assert 'semantic_chunked_pages' in globals() and len(semantic_chunked_pages) > 0, \\\n",
        " \"Run your semantic chunking code first to define 'semantic_chunked_pages'.\"\n",
        "show_random_semantic_chunks(semantic_chunked_pages, k=5, seed=42)"
      ],
      "metadata": {
        "id": "WuDeYAUUs82V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Chunking Strategy 3: Recursive chunking**\n",
        "keypoints\n",
        "\n",
        "\n",
        "*   Recursive chunking prioritizes natural text boun daries : section>paragraph> sentence\n",
        "*   It only splits further when necessary to respect the size constants\n",
        "*   Compared to fixed-sized chuning it avoids breaking words mid-way and produces more coherant chunks\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KVrqcO3Dwqp5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from tqdm.auto import tqdm\n",
        "nltk.download(\"punkt\")\n",
        "\n",
        "def recursive_chunk_text(text: str, max_chunk_size: int = 1000, min_chunk_size: int = 100) -> list:\n",
        "    \"\"\"\n",
        "    Recursively splits a block of text into chunks that  fit within size constarints.\n",
        "    tries splitting by section, then newlines, then sentences\n",
        "    \"\"\"\n",
        "    def split_chunk(chunk: str) -> list:\n",
        "        #Base case\n",
        "        if len(chunk) <= max_chunk_size:\n",
        "            return [chunk]\n",
        "\n",
        "        #try splitting by double newlines\n",
        "        sections = chunk.split(\"\\n\\n\") # chunk by double new lines\n",
        "        if len(sections) > 1:\n",
        "            result = []\n",
        "            for section in sections:\n",
        "                if section.strip():\n",
        "                    result.extend(split_chunk(section.strip()))\n",
        "            return result\n",
        "\n",
        "        #try splitting by single newline\n",
        "        sections = chunk.split(\"\\n\") #2nd recurssion is single new line\n",
        "        if len(sections) > 1:\n",
        "            result = []\n",
        "            for section in sections:\n",
        "                if section.strip():\n",
        "                    result.extend(split_chunk(section.strip()))\n",
        "            return result\n",
        "\n",
        "        #Fall back: try splitting by sentences\n",
        "        sentences = nltk.sent_tokenize(chunk) # final recursion is sentence\n",
        "        chunks, current_chunk, current_size = [], [], 0\n",
        "\n",
        "        for sentence in sentences:\n",
        "            if current_size + len(sentence) > max_chunk_size:\n",
        "                if current_chunk:\n",
        "                    chunks.append(\" \".join(current_chunk))\n",
        "                current_chunk = [sentence]\n",
        "                current_size = len(sentence)\n",
        "            else:\n",
        "                current_chunk.append(sentence)\n",
        "                current_size += len(sentence)\n",
        "        if current_chunk:\n",
        "            chunks.append(\" \".join(current_chunk))\n",
        "\n",
        "        return chunks\n",
        "\n",
        "    return split_chunk(text)\n",
        "\n",
        "def recursive_chunk_pdf_pages(pages_and_texts: list, max_chunk_size: int = 1000, min_chunk_size: int = 100) -> list[dict]:\n",
        "\n",
        "    \"\"\"\n",
        "    Takes pdf pages with text and splits then into recursive chunks.\n",
        "\n",
        "    Returns a list of dicts with page_number, chunk_index, and chunk_text.\n",
        "    \"\"\"\n",
        "\n",
        "    all_chunks = []\n",
        "\n",
        "    for page in tqdm(pages_and_texts, desc=\"Recursive chunking pages\"):\n",
        "        page_text = page[\"text\"]\n",
        "        page_number = page[\"page_number\"]\n",
        "\n",
        "        chunks = recursive_chunk_text(page_text, max_chunk_size=max_chunk_size, min_chunk_size=min_chunk_size)\n",
        "\n",
        "        for i, chunk in enumerate(chunks):\n",
        "            all_chunks.append({\n",
        "                \"page_number\": page_number,\n",
        "                \"chunk_index\": i,\n",
        "                \"chunk_char_count\": len(chunk),\n",
        "                \"chunk_word_count\": len(chunk.split()),\n",
        "                \"chunk_token_count\": len(chunk) // 4,\n",
        "                \"chunk_text\": chunk\n",
        "            })\n",
        "    return all_chunks"
      ],
      "metadata": {
        "id": "ndce-Af6wzYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "recursive_chunked_pages = recursive_chunk_pdf_pages(pages_and_texts, max_chunk_size=800, min_chunk_size=100)\n",
        "print(f\"Total recursive chunks: {len(recursive_chunked_pages)}\")\n",
        "print(f\"First recursive chunk (page {recursive_chunked_pages[0]['page_number']}):\")\n",
        "print(recursive_chunked_pages[0]['chunk_text'][:200] + \"...\")\n"
      ],
      "metadata": {
        "id": "scnP7AEaRuE-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pretty-print 5 random RECURSIVE chunks (uses 'recursive_chunked_pages)\n",
        "import random\n",
        "import textwrap # Import necessary modules: 'random' for sampling, 'textwrap' for text formatting\n",
        "\n",
        "#---Sampling & Pretty Prinnting---\n",
        "def _scattered_indices(n: int, k: int, jitter_frac: float = 0.08) -> list[int]:\n",
        "    \"\"\"Evenly spaced anchors + random jitter -indices scattered across [0, n-1], \"\"\"\n",
        "    # Check if we need to select any indices. If k (number of indices) is zero or less, return an empty list.\n",
        "    if k <= 0:\n",
        "      return []\n",
        "    # If we only need one index, pick a single random index from 0 to n-1.\n",
        "    if k == 1:\n",
        "      return [random.randrange(n)]\n",
        "    # Calculate k evenly spaced anchor points across the range [0, n-1].\n",
        "    anchors = [int(round(i * (n-1) / (k-1))) for i in range(k)]\n",
        "    out, seen = [], set()\n",
        "    radius = max(1, int(n * jitter_frac))\n",
        "\n",
        "    # Iterate through each anchor point to add random jitter.\n",
        "    for a in anchors:\n",
        "        # Define the lower (lo) and upper (hi) bounds for the random jitter, constrained by [0, n-1].\n",
        "        lo, hi = max(0, a- radius), min(n-1, a+ radius)\n",
        "        j = random.randint(lo, hi)\n",
        "\n",
        "        # If the jittered index 'j' hasn't been used yet, add it to the output and the 'seen' set.\n",
        "        if j not in seen:\n",
        "            out.append(j); seen.add(j)\n",
        "    # If the process above resulted in fewer than k unique indices (due to jitter overlap), fill the rest randomly.\n",
        "    while len(out) < k:\n",
        "        r = random.randrange(n) # Pick a random index from the whole range [0, n-1].\n",
        "        # If the index hasn't been used, add it to the output.\n",
        "        if r not in seen:\n",
        "            out.append(r); seen.add(r)\n",
        "    return out\n",
        "\n",
        "def _draw_boxed_chunk(c: dict, wrap_at: int = 96) -> str:\n",
        "    header = (\n",
        "        f\"Page p{c['page_number']} . idx {c['chunk_index']} | \"\n",
        "        f\"chunk {c['chunk_char_count']} . words {c['chunk_word_count']} . ~tokens {c['chunk_token_count']} \"\n",
        "    )\n",
        "    #wrap body text, avoid breaking long words awkwardly\n",
        "    wrapped_lines = textwrap.wrap(c[\"chunk_text\"], width=wrap_at, break_long_words=False, replace_whitespace=False\n",
        "                                  )\n",
        "    context_width = max( [0, *map(len, wrapped_lines)])\n",
        "    box_width = max(len(header), context_width + 2) # +2 for side padding\n",
        "\n",
        "    # Transcribed code starts here:\n",
        "    top = \"╔\" + \"=\" * box_width + \"╗\"\n",
        "    hline = \"║ \" + header.ljust(box_width) + \" ║\"\n",
        "    sep = \"╠\" + \"-\" * box_width + \"╣\"\n",
        "    body = \"\\n\".join(\n",
        "        \"║ \" + line.ljust(box_width - 2) + \" ║\" for line in wrapped_lines) or (\"║ \" + \"\".ljust(box_width- 2) + \" ║\")\n",
        "    bottom = \"╚\" + \"=\" * box_width + \"╝\"\n",
        "    return \"\\n\".join([top, hline, sep, body, bottom])\n",
        "def show_random_recursive_chunks(recursive_chunked_pages: list[dict], k: int = 5, seed: int | None = 42):\n",
        "    if seed is not None:\n",
        "       random.seed(seed)\n",
        "    n = len(recursive_chunked_pages)\n",
        "    #if n == 0:\n",
        "        #print(\"No semantic chunks to display.\");\n",
        "        #return\n",
        "    assert n > 0, \"No recursive chunks to display. Did you run the recursive chunking cell?\"\n",
        "    idxs = _scattered_indices(n, k)\n",
        "    print(f\"Showing {len(idxs)} scattered random RECURSINE chunks out of {n} total:\\n\")\n",
        "    for i, idx in enumerate(idxs, 1):\n",
        "        print(f\"#{i}\")\n",
        "        print(_draw_boxed_chunk(recursive_chunked_pages[idx]))\n",
        "        print()\n",
        "\n",
        "# run (expects youo have already created semantic chunked pages)\n",
        "assert 'recursive_chunked_pages' in globals() and len(recursive_chunked_pages) > 0, \\\n",
        " \"Run your recursive chunking code first to define 'recursive_chunked_pages'.\"\n",
        "show_random_recursive_chunks(recursive_chunked_pages, k=5, seed=42)\n",
        "\n"
      ],
      "metadata": {
        "id": "lrbYXJU5SfSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Chunking strategy 4 : Document structure based chunking**\n",
        "How structure based chunking works?\n",
        "\n",
        "\n",
        "*   The function looks for headers such as chapter numbers(eg. chapter 1) and section heading(1.1. Introduction)\n",
        "*   Every time it finds a header, it starts a new chunk\n",
        "\n",
        "\n",
        "*   Text is grouped with its closest heading until the nex heading is reached or the chunk size is exceeded\n",
        "*   This preserves the logical flow of a textbook, where content under each heading remains together.\n",
        "\n",
        "**Structure based chunking** is usefull for documents with a clear hierarchy(chapters,sections, subsections,). Unlike fixed-size chunking, it ensures that text remains tied to its heading, improving coherence and preserving semantic meaning.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "niZDg3HScbQZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#--- chapter -based chunking (simple & fast)----\n",
        "# Assumes you have already run your base pdf so 'pages_and_texts' exists.\n",
        "#we detect a new chapter whenever a page contains \"University of Hawai\" header.\n",
        "# Each chapter = pages from one header until the p[age before the next header\n",
        "\n",
        "\n",
        "import re  # regexr for text matching. ! which is regular expression\n",
        "import random\n",
        "import textwrap\n",
        "\n",
        "# 1) helper to detect \"chapter start\" pages\n",
        "def _is_chater_header_page(text: str) -> bool:\n",
        "    # Robust to punctuations/diacritiics differences; matches the recurring header\n",
        "    # e,g \"University of Hawai I at Manda food science and human nutriyion program\"\n",
        "    return re.search(r\"University\\s+of\\s+hawai\", text, flags=re.IGNORECASE) is not None\n",
        "\n",
        "def _guess_title_from_header(header: str) -> str:\n",
        "    \"\"\"\n",
        "    Best-effort chapter title guess = the text before the 'University of Hawai' header line.\n",
        "    Falls back to the first ~120 character.\n",
        "    \"\"\"\n",
        "    m = re.search(r\"University\\s+of\\s+hawai\", header, flags=re.IGNORECASE)\n",
        "    if m:\n",
        "        title = header[:m.start()].strip()\n",
        "        #keep it readable\n",
        "        title = re.sub(r\"\\s+\", \" \", title).strip()\n",
        "        if 10 <= len(title) <= 180:\n",
        "            return title\n",
        "    #fallback\n",
        "    t = re.sub(r\"\\s+\", \" \", header).strip()\n",
        "    return t[:120] if t else \"Untitled chapter\"\n",
        "\n",
        "# 2) Biuld chapter chunks\n",
        "def chapter_chunk_pdf_pages(pages_and_texts: list[dict]) -> list[dict]:\n",
        "    \"\"\"\n",
        "    Returns a list of chapter chunks:\n",
        "    [\n",
        "        'chapter_index' int,\n",
        "        'title' str,\n",
        "        'page_end': int,\n",
        "        'chunk_char_count': int,\n",
        "        'chunk_word_count': int,\n",
        "        'chunk_token_count': float,\n",
        "        'chunk_text': str\n",
        "    ]\n",
        "    \"\"\"\n",
        "    if not pages_and_texts:\n",
        "        return []\n",
        "\n",
        "    # Find all page indices that look like the start of a chapter\n",
        "    chapter_starts = []\n",
        "    for i, p in enumerate(pages_and_texts):\n",
        "        txt = p[\"text\"]\n",
        "        if _is_chater_header_page(txt):\n",
        "            chapter_starts.append(i)\n",
        "\n",
        "    # If nothing detected, return empty (or treat entire doc as one chunk)\n",
        "    if not chapter_starts:\n",
        "        #treat entire doc as one \"chapter\"\n",
        "        all_text =\" \".join(p[\"text\"] for p in pages_and_texts).strip()\n",
        "        return [{\n",
        "            \"chapter_index\": 0,\n",
        "            \"title\": _guess_title_from_header(pages_and_texts[0][\"text\"]),\n",
        "            \"page_start\": pages_and_texts[0][\"page_number\"],\n",
        "            \"page_end\": pages_and_texts[- 1][\"page_number\"],\n",
        "            \"chunk_char_count\": len(all_text),\n",
        "            \"chunk_word_count\": len(all_text.split()),\n",
        "            \"chunk_token_count\": round(len(all_text) / 4, 2),\n",
        "            \"chunk_text\": all_text\n",
        "        }]\n",
        "\n",
        "    #Build chapter ranges (start -> next_start-1)\n",
        "    # where ever this \"University\\s+of\\s+hawai\" apperas it will be start of chunk and end of the chunk\n",
        "    chapter_chunks = []\n",
        "    for ci, s in enumerate(chapter_starts):\n",
        "        e = (chapter_starts[ci + 1]-1) if (ci + 1 < len(chapter_starts)) else len(pages_and_texts)-1\n",
        "        if e < 5:\n",
        "            continue #gaurd (should n't happend)\n",
        "\n",
        "        pages = pages_and_texts[s:e + 1]\n",
        "        text_concat = \" \".join(p[\"text\"] for p in pages).strip()\n",
        "        title = _guess_title_from_header(pages[0][\"text\"])\n",
        "\n",
        "        chapter_chunks.append({\n",
        "            \"chapter_index\": ci,\n",
        "            \"title\": title,\n",
        "            \"page_start\": pages[0][\"page_number\"],\n",
        "            \"page_end\": pages[-1][\"page_number\"],\n",
        "            \"chunk_char_count\": len(text_concat),\n",
        "            \"chunk_word_count\": len(text_concat.split()),\n",
        "            \"chunk_token_count\": round(len(text_concat) / 4, 2),\n",
        "            \"chunk_text\": text_concat\n",
        "        })\n",
        "    return chapter_chunks\n",
        "\n",
        "\n",
        "#"
      ],
      "metadata": {
        "id": "HnbXt8h-dZbt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "structure_chunked_pages = chapter_chunk_pdf_pages(pages_and_texts)\n",
        "\n",
        "print(f\"Total chpater-based chunks: {len(structure_chunked_pages)}\")\n",
        "if structure_chunked_pages:\n",
        "    first = structure_chunked_pages[0]\n",
        "    print(f\"First chapter (pages {first['page_start']}-{first['page_end']}): {first['title']}\")\n",
        "    print(first['chunk_text'][:200] +\"...\")\n",
        "else:\n",
        "    print(\"No chapter-based chunks detected.\")"
      ],
      "metadata": {
        "id": "D4v9c7yc4dbr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _draw_boxed_chunk(c: dict, wrap_at: int = 96) -> str:\n",
        "    header = (\n",
        "        f\"Page p{c.get('page_start', 'N/A')}-{c.get('page_end', 'N/A')} . idx {c.get('chapter_index', 'N/A')} | \" # Use get with default for robustness\n",
        "        f\"chunk {c['chunk_char_count']} . words {c['chunk_word_count']} . ~tokens {c['chunk_token_count']} \"\n",
        "    )\n",
        "    #wrap body text, avoid breaking long words awkwardly\n",
        "    wrapped_lines = textwrap.wrap(c[\"chunk_text\"], width=wrap_at, break_long_words=False, replace_whitespace=False\n",
        "                                  )\n",
        "    context_width = max( [0, *map(len, wrapped_lines)])\n",
        "    box_width = max(len(header), context_width + 2) # +2 for side padding\n",
        "\n",
        "    # Transcribed code starts here:\n",
        "    top = \"╔\" + \"=\" * box_width + \"╗\"\n",
        "    hline = \"║ \" + header.ljust(box_width) + \" ║\"\n",
        "    sep = \"╠\" + \"-\" * box_width + \"╣\"\n",
        "    body = \"\\n\".join(\n",
        "        \"║ \" + line.ljust(box_width - 2) + \" ║\" for line in wrapped_lines) or (\"║ \" + \"\".ljust(box_width- 2) + \" ║\")\n",
        "    bottom = \"╚\" + \"=\" * box_width + \"╝\"\n",
        "    return \"\\n\".join([top, hline, sep, body, bottom])\n",
        "def show_random_chapter_chunks(chapter_chunks: list[dict], k: int = 5, seed: int | None = 42):\n",
        "    if not chapter_chunks:\n",
        "        print(\"No chapter-based chunks to display.\");\n",
        "        return\n",
        "    if seed is not None:\n",
        "       random.seed(seed)\n",
        "    #n = len(recursive_chunked_pages)\n",
        "    #if n == 0:\n",
        "        #print(\"No semantic chunks to display.\");\n",
        "        #return\n",
        "    #assert n > 0, \"No recursive chunks to display. Did you run the recursive chunking cell?\"\n",
        "    k = min(k, len(chapter_chunks))\n",
        "    idxs = random.sample(range(len(chapter_chunks)), k)\n",
        "    print(f\"Showing {len(idxs)} random chapter chunks out of {len(chapter_chunks)} total:\\n\")\n",
        "    for i, idx in enumerate(idxs, 1):\n",
        "        print(f\"#{i}\")\n",
        "        print(_draw_boxed_chunk(chapter_chunks[idx]))\n",
        "        print()\n",
        "\n",
        "# 4) run (expects youo have already created semantic chunked pages)\n",
        "assert 'pages_and_texts' in globals(), \"Run your base pdf code first to define pages_and_texts\"\n",
        "chapter_chunks = chapter_chunk_pdf_pages(pages_and_texts)\n",
        "print(f\"Total chapters detected: {len(chapter_chunks)}\")\n",
        "if chapter_chunks:\n",
        "    print(f\"First chapter: {chapter_chunks[0]['title']} (p{chapter_chunks[0]['page_start']}-{chapter_chunks[0]['page_end']})\")\n",
        "\n",
        "#Inspects a few\n",
        "show_random_chapter_chunks(chapter_chunks, k=5, seed=21)"
      ],
      "metadata": {
        "id": "b9rV7m5Z6c-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Chunking Strategy 5: LLM based chunking**\n",
        "This chuning strategy uses an LLM to create semantically coherant chunks by understanding context and maintaining thematic consistency through natural language processing.\n",
        "Here we use an API key."
      ],
      "metadata": {
        "id": "tK6F7HTL_Pdw"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_w4GOSQAAN78"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}